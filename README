For the moment, there's nothing much to see here...

Currently, I use a heuristic with the following features:
* First, add the parent repositories of repositories that the user is already watching, in order of popularity
* Then, those other repositories by the same author as the current one
* Then, add ancestor repositories (ie, more than one parent away)
* Finally, add in order of popularity

By "order of popularity", I mean in order of the number of users that are watching.

Thanks to:
* Daniel Haran's code and README for the idea of using parent repositories;
* asciiarmor's code (thanks for publishing it) for the basic features necessary to get to reasonable performance

Future things that I will try:
* Use a classifier instead of a heuristic to rank the set of potential repositories (this will help us where the user has a lot of watched repositories);
* Look at language matches;
* Look at keywords in the repository names (eg, "rails" says a lot about which kind of repos that people like)
* Add a simple knn implementation based upon a dot product between repos to find new ones
* EM on a graphical model to learn a smoothed user/repo matrix
* Embed both repositories and users into the same feature space using a deep neural network and use cosine similarity between embedded representations to learn similarity.
* Potentially, other (simpler) matrix factorization models
* Blending, of course, once I have enough models to make it useful

Feel free to browse my code and take my ideas for your system, if ever they are useful.

If ever you want to use my code, you can do so under the Affero GPL v3.  Note that getting it to work might be difficult; it relies on JML (Jeremy's Machine Learning Library) at http://bitbucket.org/jeremy_barnes/jml/.  This code needs to be downloaded and compiled and the jml symlink in the github repo set to point to it.  I build it on 32 and 64 bit Ubuntu 8.10/9.04 machines; anything else might be difficult.

I'm currently looking for consulting contracts.  I do pretty much any data mining, machine learning or early-stage technology stuff, but I'm particularly experienced in Natural Language Processing.

You can contact me on <my firstname> at <my family name>+"o" .com.

Jeremy Barnes
8 August 2009

==================

Notes

Before also watched:

fake test results: 
     total:      real: 2286/4788 =  47.74%  poss: 2679/4788 =  55.95%
     enough:     real: 1397/3348 =  41.73%  poss: 1811/3348 =  54.09%  avg num: 300.9
     not enough: real:  889/1440 =  61.74%  poss:  868/1440 =  60.28%  avg num:   3.7

After also watched:

fake test results: 
     total:      real: 2289/4788 =  47.81%  poss: 3014/4788 =  62.95%
     enough:     real: 1431/3383 =  42.30%  poss: 2177/3383 =  64.35%  avg num: 1481.3
     not enough: real:  858/1405 =  61.07%  poss:  837/1405 =  59.57%  avg num:   3.7

So we moved a lot from impossible to possible, but they didn't tend to be the highest ones.
